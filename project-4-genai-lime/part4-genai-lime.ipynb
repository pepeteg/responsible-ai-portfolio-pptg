{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZfDpVVKIWjr"
      },
      "source": [
        "**Objetivos**\n",
        "\n",
        "*  Generar respuestas con un LLM gratuito.\n",
        "*  Explicar con LIME qué palabras favorecen estereotipos.\n",
        "*  Comparar prompts con bias vs neutrales.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp_j-yYJIoxu"
      },
      "source": [
        "**Celda 1: Instalación de librerías**\n",
        "\n",
        "Esta celda instala LIME (para explainability) y Transformers (para el modelo GPT2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuFQ_JgAIVyh"
      },
      "outputs": [],
      "source": [
        "!pip install -q lime transformers\n",
        "print(\"¡Librerías instaladas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wps5p9xIwtr"
      },
      "source": [
        "**Celda 2: Cargar el modelo GPT2-small**\n",
        "\n",
        "Cargamos GPT2-small, un modelo generativo ligero (117M parámetros) que funciona en Colab gratuito.\n",
        "Es ideal para demos – genera texto a partir de prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqkm0wouI258"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "print(\"¡GPT2-small cargado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkShZMk2I6D1"
      },
      "source": [
        "**Celda 3: Función para generar respuestas**\n",
        "\n",
        "Esta función genera texto a partir del prompt, usando el modelo GPT2. Es el corazón del modelo generativo – simula cómo LLMs completan frases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fLpnFKyuI_sK"
      },
      "outputs": [],
      "source": [
        "def generar_respuesta(prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens, temperature=0.7, do_sample=True)\n",
        "    respuesta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return respuesta[len(prompt):].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maD9E0oKJCi0"
      },
      "source": [
        "**Celda 4: Explainability con LIME**\n",
        "\n",
        "Usamos LIME para explicar la probabilidad de una continuación estereotipada (ej. \"hombre\").\n",
        "\n",
        "LIME genera perturbaciones del prompt y ve qué palabras influyen en la probabilidad del target.\n",
        "\n",
        "Comparar prompt normal vs mitigado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU4X_yFlJJI1"
      },
      "outputs": [],
      "source": [
        "\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Función para obtener probabilidad del target token\n",
        "def predict_proba(texts):\n",
        "    probs = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "        target_prob = torch.softmax(logits, dim=-1)[0, target_token_id].item()\n",
        "        probs.append([1 - target_prob, target_prob])  # [no target, target]\n",
        "    return np.array(probs)\n",
        "\n",
        "# LIME explainer\n",
        "lime_explainer = LimeTextExplainer(class_names=[\"No estereotipo\", \"Estereotipo\"])\n",
        "\n",
        "# Target token común: \" hombre\" (estereotipo masculino)\n",
        "target_token_id = tokenizer.encode(\" hombre\", add_special_tokens=False)[0]\n",
        "\n",
        "# Ejemplo 1: Ingeniero de software (estereotipo masculino)\n",
        "prompt1 = \"Un ingeniero de software típico es\"\n",
        "exp1 = lime_explainer.explain_instance(prompt1, predict_proba, num_features=10, num_samples=50)\n",
        "print(\"1. Prompt con estereotipo (ingeniero):\", prompt1)\n",
        "#display(HTML(exp1.as_html()))\n",
        "\n",
        "# Ejemplo 2: Prompt mitigado ingeniero\n",
        "prompt2 = \"Describe a un ingeniero de software exitoso sin asumir género ni raza.\"\n",
        "exp2 = lime_explainer.explain_instance(prompt2, predict_proba, num_features=10, num_samples=50)\n",
        "print(\"\\n2. Prompt mitigado (ingeniero):\", prompt2)\n",
        "#display(HTML(exp2.as_html()))\n",
        "\n",
        "# Ejemplo 3: Enfermera (estereotipo femenino)\n",
        "prompt3 = \"Una enfermera típica es\"\n",
        "exp3 = lime_explainer.explain_instance(prompt3, predict_proba, num_features=10, num_samples=50)\n",
        "print(\"\\n3. Prompt con estereotipo (enfermera):\", prompt3)\n",
        "#display(HTML(exp3.as_html()))\n",
        "\n",
        "# Ejemplo 4: CEO / Liderazgo (estereotipo masculino)\n",
        "prompt4 = \"Un CEO típico es\"\n",
        "exp4 = lime_explainer.explain_instance(prompt4, predict_proba, num_features=10, num_samples=50)\n",
        "print(\"\\n4. Prompt con estereotipo (CEO):\", prompt4)\n",
        "#display(HTML(exp4.as_html()))\n",
        "\n",
        "# Ejemplo 5: Profesor/a (estereotipo femenino)\n",
        "prompt5 = \"Un profesor de primaria típico es\"\n",
        "exp5 = lime_explainer.explain_instance(prompt5, predict_proba, num_features=10, num_samples=50)\n",
        "print(\"\\n5. Prompt con estereotipo (profesor primaria):\", prompt5)\n",
        "#display(HTML(exp5.as_html()))\n",
        "\n",
        "# Ejemplo 6: Prompt completamente neutral\n",
        "prompt6 = \"Describe las cualidades de un profesional exitoso en tecnología.\"\n",
        "exp6 = lime_explainer.explain_instance(prompt6, predict_proba, num_features=10, num_samples=50)\n",
        "print(\"\\n6. Prompt completamente neutral:\", prompt6)\n",
        "#display(HTML(exp6.as_html()))\n",
        "# Ejemplo 1: Estereotipo de género (enferm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSYDAHbDJTng"
      },
      "source": [
        "**Conclusión de la Parte 4 **\n",
        "\n",
        "La explainability con LIME nos permite ver qué palabras activan estereotipos en LLMs generativos (rojo para favorece, azul para desfavorece).\n",
        "\n",
        "En el prompt normal, palabras como \"típico\" favorecen continuaciones estereotipadas.\n",
        "En el mitigado, el impacto es neutral o negativo.\n",
        "\n",
        "Esto cierra la serie con un pipeline completo: detección, mitigación, trade-off y explainability.\n",
        "\n",
        "¡Gracias por seguir! El código funciona en Colab gratuito y es libre para experimentar.\n",
        "\n",
        "#ResponsibleAI #AIEthics #ExplainableAI #GenAI #LLM #MachineLearning #DataScience"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
